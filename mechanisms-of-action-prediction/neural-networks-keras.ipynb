{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:40.138632Z",
     "iopub.status.busy": "2020-11-23T00:52:40.137414Z",
     "iopub.status.idle": "2020-11-23T00:52:40.143362Z",
     "shell.execute_reply": "2020-11-23T00:52:40.142515Z"
    },
    "papermill": {
     "duration": 0.056156,
     "end_time": "2020-11-23T00:52:40.143528",
     "exception": false,
     "start_time": "2020-11-23T00:52:40.087372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:40.229658Z",
     "iopub.status.busy": "2020-11-23T00:52:40.228296Z",
     "iopub.status.idle": "2020-11-23T00:52:50.160876Z",
     "shell.execute_reply": "2020-11-23T00:52:50.160051Z"
    },
    "papermill": {
     "duration": 9.981652,
     "end_time": "2020-11-23T00:52:50.161011",
     "exception": false,
     "start_time": "2020-11-23T00:52:40.179359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from skmultilearn.model_selection.iterative_stratification import IterativeStratification, iterative_train_test_split\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036039,
     "end_time": "2020-11-23T00:52:50.233603",
     "exception": false,
     "start_time": "2020-11-23T00:52:50.197564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "https://www.kaggle.com/elcaiseri/moa-keras-multilabel-classifier-nn-starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037751,
     "end_time": "2020-11-23T00:52:50.307815",
     "exception": false,
     "start_time": "2020-11-23T00:52:50.270064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:50.394255Z",
     "iopub.status.busy": "2020-11-23T00:52:50.392866Z",
     "iopub.status.idle": "2020-11-23T00:52:57.901880Z",
     "shell.execute_reply": "2020-11-23T00:52:57.901020Z"
    },
    "papermill": {
     "duration": 7.557034,
     "end_time": "2020-11-23T00:52:57.902033",
     "exception": false,
     "start_time": "2020-11-23T00:52:50.344999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/lish-moa/'\n",
    "\n",
    "train_features = pd.read_csv(os.path.join(base_path, 'train_features.csv'))\n",
    "test_features = pd.read_csv(os.path.join(base_path, 'test_features.csv'))\n",
    "\n",
    "train_targets = pd.read_csv(os.path.join(base_path, 'train_targets_scored.csv'))\n",
    "train_targets_nonscored = pd.read_csv(os.path.join(base_path, 'train_targets_nonscored.csv'))\n",
    "submission = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037753,
     "end_time": "2020-11-23T00:52:57.976580",
     "exception": false,
     "start_time": "2020-11-23T00:52:57.938827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:58.061033Z",
     "iopub.status.busy": "2020-11-23T00:52:58.060121Z",
     "iopub.status.idle": "2020-11-23T00:52:58.064503Z",
     "shell.execute_reply": "2020-11-23T00:52:58.063759Z"
    },
    "papermill": {
     "duration": 0.051565,
     "end_time": "2020-11-23T00:52:58.064639",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.013074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})    \n",
    "    #df = pd.get_dummies(df, columns=['cp_time','cp_dose'])\n",
    "    \n",
    "    del df['sig_id']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:58.146061Z",
     "iopub.status.busy": "2020-11-23T00:52:58.145266Z",
     "iopub.status.idle": "2020-11-23T00:52:58.461855Z",
     "shell.execute_reply": "2020-11-23T00:52:58.461156Z"
    },
    "papermill": {
     "duration": 0.360099,
     "end_time": "2020-11-23T00:52:58.461993",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.101894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x = preprocess_data(train_features)\n",
    "train_y = train_targets.copy()\n",
    "train_y.drop('sig_id', axis=1, inplace=True)\n",
    "train_y_nonscored = train_targets_nonscored.copy()\n",
    "train_y_nonscored.drop('sig_id', axis=1, inplace=True)\n",
    "test = preprocess_data(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:58.548235Z",
     "iopub.status.busy": "2020-11-23T00:52:58.546700Z",
     "iopub.status.idle": "2020-11-23T00:52:58.611727Z",
     "shell.execute_reply": "2020-11-23T00:52:58.610827Z"
    },
    "papermill": {
     "duration": 0.112351,
     "end_time": "2020-11-23T00:52:58.611881",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.499530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take out targets with no activations in the nonscored target table\n",
    "\n",
    "target_act_count = train_y_nonscored.sum(axis=0).sort_values()\n",
    "zeros_targets = list(target_act_count[target_act_count==0].index)\n",
    "train_y_nonscored = train_y_nonscored[[c for c in train_y_nonscored.columns if c not in zeros_targets]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036729,
     "end_time": "2020-11-23T00:52:58.685414",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.648685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036231,
     "end_time": "2020-11-23T00:52:58.760378",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.724147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stats from features\n",
    "\n",
    "https://www.kaggle.com/ragnar123/moa-dnn-feature-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:58.869697Z",
     "iopub.status.busy": "2020-11-23T00:52:58.862580Z",
     "iopub.status.idle": "2020-11-23T00:52:58.875583Z",
     "shell.execute_reply": "2020-11-23T00:52:58.874901Z"
    },
    "papermill": {
     "duration": 0.077321,
     "end_time": "2020-11-23T00:52:58.875728",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.798407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract common stats features\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = [c for c in train.columns if 'g-' in c]\n",
    "    features_c = [c for c in train.columns if 'c-' in c]\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def c_squared(train, test):\n",
    "    \n",
    "    features_c = [c for c in train.columns if 'c-' in c]\n",
    "    for df in [train, test]:\n",
    "        for feature in features_c:\n",
    "            feature_alt = feature.replace('-','_')\n",
    "            df[f'{feature_alt}_squared'] = df[feature] ** 2\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n",
    "    \n",
    "    features_g = [c for c in train.columns if 'g-' in c]\n",
    "    features_c = [c for c in train.columns if 'c-' in c]\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:52:59.024820Z",
     "iopub.status.busy": "2020-11-23T00:52:59.024001Z",
     "iopub.status.idle": "2020-11-23T00:53:52.602356Z",
     "shell.execute_reply": "2020-11-23T00:53:52.601557Z"
    },
    "papermill": {
     "duration": 53.690449,
     "end_time": "2020-11-23T00:53:52.602519",
     "exception": false,
     "start_time": "2020-11-23T00:52:58.912070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, test = fe_cluster(train_x, test)\n",
    "train_x, test = fe_stats(train_x, test)\n",
    "train_x, test = c_squared(train_x, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036524,
     "end_time": "2020-11-23T00:53:52.675990",
     "exception": false,
     "start_time": "2020-11-23T00:53:52.639466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036664,
     "end_time": "2020-11-23T00:53:52.748767",
     "exception": false,
     "start_time": "2020-11-23T00:53:52.712103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RankGauss\n",
    "\n",
    "- This was showing better performance compare to MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:53:52.839523Z",
     "iopub.status.busy": "2020-11-23T00:53:52.838383Z",
     "iopub.status.idle": "2020-11-23T00:54:05.959088Z",
     "shell.execute_reply": "2020-11-23T00:54:05.959771Z"
    },
    "papermill": {
     "duration": 13.175293,
     "end_time": "2020-11-23T00:54:05.959938",
     "exception": false,
     "start_time": "2020-11-23T00:53:52.784645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_x.columns if 'cp_' not in c]\n",
    "\n",
    "for col in features:\n",
    "\n",
    "    transformer = preprocessing.QuantileTransformer(n_quantiles=100,random_state=0,\n",
    "                                                    output_distribution=\"normal\")\n",
    "    vec_len = len(train_x[col].values)\n",
    "    vec_len_test = len(test[col].values)\n",
    "    raw_vec = train_x[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_x[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042994,
     "end_time": "2020-11-23T00:54:06.040266",
     "exception": false,
     "start_time": "2020-11-23T00:54:05.997272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:06.130724Z",
     "iopub.status.busy": "2020-11-23T00:54:06.129604Z",
     "iopub.status.idle": "2020-11-23T00:54:06.133170Z",
     "shell.execute_reply": "2020-11-23T00:54:06.132311Z"
    },
    "papermill": {
     "duration": 0.050156,
     "end_time": "2020-11-23T00:54:06.133315",
     "exception": false,
     "start_time": "2020-11-23T00:54:06.083159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# scaler.fit(train_x)\n",
    "\n",
    "# train_trans = scaler.transform(train_x)\n",
    "# test_trans = scaler.transform(test)\n",
    "\n",
    "# train_x = pd.DataFrame(train_trans, columns=train_x.columns)\n",
    "# test = pd.DataFrame(test_trans, columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041799,
     "end_time": "2020-11-23T00:54:06.215831",
     "exception": false,
     "start_time": "2020-11-23T00:54:06.174032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PCA\n",
    "\n",
    "- During the EDA process, we saw a lot of correlations among the gene and cell features (especially cell), so we'll use PCA to reduce our feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:06.306027Z",
     "iopub.status.busy": "2020-11-23T00:54:06.304152Z",
     "iopub.status.idle": "2020-11-23T00:54:08.844730Z",
     "shell.execute_reply": "2020-11-23T00:54:08.844007Z"
    },
    "papermill": {
     "duration": 2.588483,
     "end_time": "2020-11-23T00:54:08.844904",
     "exception": false,
     "start_time": "2020-11-23T00:54:06.256421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cumulative explained variance')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g_features = [c for c in train_x.columns if 'g-' in c]\n",
    "g_feature_train = train_x[g_features].copy()\n",
    "\n",
    "pca = PCA().fit(g_feature_train)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:08.941903Z",
     "iopub.status.busy": "2020-11-23T00:54:08.940982Z",
     "iopub.status.idle": "2020-11-23T00:54:08.944554Z",
     "shell.execute_reply": "2020-11-23T00:54:08.943804Z"
    },
    "papermill": {
     "duration": 0.055111,
     "end_time": "2020-11-23T00:54:08.944680",
     "exception": false,
     "start_time": "2020-11-23T00:54:08.889569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_pca(pca, feature_df, col_type='g'):\n",
    "    \n",
    "    projected_df = pca.transform(feature_df)\n",
    "    projected_df = pd.DataFrame(projected_df)\n",
    "    projected_df.columns = [col_type+'_component_' + str(n) for n in projected_df.columns]\n",
    "    \n",
    "    return projected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:09.094947Z",
     "iopub.status.busy": "2020-11-23T00:54:09.085891Z",
     "iopub.status.idle": "2020-11-23T00:54:11.910758Z",
     "shell.execute_reply": "2020-11-23T00:54:11.910090Z"
    },
    "papermill": {
     "duration": 2.92494,
     "end_time": "2020-11-23T00:54:11.910907",
     "exception": false,
     "start_time": "2020-11-23T00:54:08.985967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for gene features to keep 95% variance:  602\n"
     ]
    }
   ],
   "source": [
    "# Keeping 95% variance\n",
    "variance = 0.95\n",
    "# gene features\n",
    "g_features = [c for c in train_x.columns if 'g-' in c]\n",
    "g_pca = PCA(variance).fit(train_x[g_features])\n",
    "print(f'Number of components for gene features to keep {int(variance*100)}% variance: ', g_pca.n_components_)\n",
    "\n",
    "transformed_g_train = apply_pca(g_pca, train_x[g_features], col_type='g')\n",
    "#train_x.drop(g_features, axis=1, inplace=True)\n",
    "train_x = pd.concat([train_x, transformed_g_train], axis=1)\n",
    "\n",
    "transformed_g_test = apply_pca(g_pca, test[g_features], col_type='g')\n",
    "#test.drop(g_features, axis=1, inplace=True)\n",
    "test = pd.concat([test, transformed_g_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:12.005693Z",
     "iopub.status.busy": "2020-11-23T00:54:12.003593Z",
     "iopub.status.idle": "2020-11-23T00:54:12.737273Z",
     "shell.execute_reply": "2020-11-23T00:54:12.737897Z"
    },
    "papermill": {
     "duration": 0.786769,
     "end_time": "2020-11-23T00:54:12.738064",
     "exception": false,
     "start_time": "2020-11-23T00:54:11.951295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for cell features to keep 95% variance:  83\n"
     ]
    }
   ],
   "source": [
    "# cell features\n",
    "c_features = [c for c in train_x.columns if 'c-' in c]\n",
    "c_pca = PCA(variance).fit(train_x[c_features])\n",
    "print(f'Number of components for cell features to keep {int(variance*100)}% variance: ', c_pca.n_components_)\n",
    "\n",
    "transformed_c_train = apply_pca(c_pca, train_x[c_features], col_type='c')\n",
    "#train_x.drop(c_features, axis=1, inplace=True)\n",
    "train_x = pd.concat([train_x, transformed_c_train], axis=1)\n",
    "\n",
    "transformed_c_test = apply_pca(c_pca, test[c_features], col_type='c')\n",
    "#test.drop(c_features, axis=1, inplace=True)\n",
    "test = pd.concat([test, transformed_c_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039846,
     "end_time": "2020-11-23T00:54:12.817993",
     "exception": false,
     "start_time": "2020-11-23T00:54:12.778147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:12.908727Z",
     "iopub.status.busy": "2020-11-23T00:54:12.907787Z",
     "iopub.status.idle": "2020-11-23T00:54:12.911340Z",
     "shell.execute_reply": "2020-11-23T00:54:12.910600Z"
    },
    "papermill": {
     "duration": 0.050304,
     "end_time": "2020-11-23T00:54:12.911493",
     "exception": false,
     "start_time": "2020-11-23T00:54:12.861189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:13.011123Z",
     "iopub.status.busy": "2020-11-23T00:54:13.010183Z",
     "iopub.status.idle": "2020-11-23T00:54:14.321492Z",
     "shell.execute_reply": "2020-11-23T00:54:14.320739Z"
    },
    "papermill": {
     "duration": 1.370579,
     "end_time": "2020-11-23T00:54:14.321639",
     "exception": false,
     "start_time": "2020-11-23T00:54:12.951060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_x.columns if 'cp_' not in c]\n",
    "\n",
    "thresh = VarianceThreshold(0.8)\n",
    "fitted = thresh.fit(train_x.loc[:, features])\n",
    "\n",
    "train_x_transformed = train_x.loc[:, features].copy()\n",
    "train_x_transformed = train_x_transformed[train_x_transformed.columns[fitted.get_support(indices=True)]]\n",
    "\n",
    "test_features_transformed = test.loc[:, features].copy()\n",
    "test_features_transformed = test_features_transformed[test_features_transformed.columns[fitted.get_support(indices=True)]]\n",
    "\n",
    "#test_features_transformed = var_fitted.transform(test.iloc[:, 3:])\n",
    "\n",
    "\n",
    "tr_features = pd.DataFrame(train_x[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\n",
    "                          columns=['cp_type','cp_time','cp_dose'])\n",
    "train_x = pd.concat([tr_features, pd.DataFrame(train_x_transformed)], axis=1)\n",
    "\n",
    "\n",
    "te_features = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\n",
    "                            columns=['cp_type','cp_time','cp_dose'])\n",
    "test = pd.concat([te_features, pd.DataFrame(test_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:14.424826Z",
     "iopub.status.busy": "2020-11-23T00:54:14.423781Z",
     "iopub.status.idle": "2020-11-23T00:54:14.443347Z",
     "shell.execute_reply": "2020-11-23T00:54:14.443931Z"
    },
    "papermill": {
     "duration": 0.080882,
     "end_time": "2020-11-23T00:54:14.444100",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.363218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>g_component_156</th>\n",
       "      <th>g_component_157</th>\n",
       "      <th>c_component_0</th>\n",
       "      <th>c_component_1</th>\n",
       "      <th>c_component_2</th>\n",
       "      <th>c_component_3</th>\n",
       "      <th>c_component_4</th>\n",
       "      <th>c_component_5</th>\n",
       "      <th>c_component_6</th>\n",
       "      <th>c_component_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.134849</td>\n",
       "      <td>0.907687</td>\n",
       "      <td>-0.416385</td>\n",
       "      <td>-0.966814</td>\n",
       "      <td>-0.254723</td>\n",
       "      <td>-1.017473</td>\n",
       "      <td>-1.364787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.964606</td>\n",
       "      <td>-0.919256</td>\n",
       "      <td>-4.919171</td>\n",
       "      <td>1.552054</td>\n",
       "      <td>1.341573</td>\n",
       "      <td>1.461339</td>\n",
       "      <td>0.787781</td>\n",
       "      <td>1.178956</td>\n",
       "      <td>-0.030037</td>\n",
       "      <td>0.090488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119282</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.272399</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>1.205169</td>\n",
       "      <td>0.686517</td>\n",
       "      <td>0.313396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.944428</td>\n",
       "      <td>0.118273</td>\n",
       "      <td>-5.092003</td>\n",
       "      <td>-0.378341</td>\n",
       "      <td>-0.181403</td>\n",
       "      <td>0.941975</td>\n",
       "      <td>-0.512484</td>\n",
       "      <td>-0.665863</td>\n",
       "      <td>-0.464563</td>\n",
       "      <td>0.449651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.779973</td>\n",
       "      <td>0.946463</td>\n",
       "      <td>1.425350</td>\n",
       "      <td>-0.132928</td>\n",
       "      <td>-0.006122</td>\n",
       "      <td>1.492493</td>\n",
       "      <td>0.235577</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.403747</td>\n",
       "      <td>0.114112</td>\n",
       "      <td>1.355263</td>\n",
       "      <td>0.297862</td>\n",
       "      <td>0.288732</td>\n",
       "      <td>-0.153444</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>0.537705</td>\n",
       "      <td>0.924253</td>\n",
       "      <td>0.060457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.734910</td>\n",
       "      <td>-0.274641</td>\n",
       "      <td>-0.438509</td>\n",
       "      <td>0.759097</td>\n",
       "      <td>2.346330</td>\n",
       "      <td>-0.858153</td>\n",
       "      <td>-2.288417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575879</td>\n",
       "      <td>-1.070684</td>\n",
       "      <td>10.936555</td>\n",
       "      <td>1.142666</td>\n",
       "      <td>1.118825</td>\n",
       "      <td>-1.398768</td>\n",
       "      <td>0.073445</td>\n",
       "      <td>1.413511</td>\n",
       "      <td>-0.048360</td>\n",
       "      <td>0.520131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.452718</td>\n",
       "      <td>-0.477513</td>\n",
       "      <td>0.972316</td>\n",
       "      <td>0.970731</td>\n",
       "      <td>1.463427</td>\n",
       "      <td>-0.869555</td>\n",
       "      <td>-0.375501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.804947</td>\n",
       "      <td>-0.989905</td>\n",
       "      <td>-3.638540</td>\n",
       "      <td>0.565012</td>\n",
       "      <td>0.700291</td>\n",
       "      <td>-0.199855</td>\n",
       "      <td>0.261442</td>\n",
       "      <td>-0.289355</td>\n",
       "      <td>0.248847</td>\n",
       "      <td>-0.226713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cp_type  cp_time  cp_dose       g-0       g-1       g-2       g-3  \\\n",
       "0        0        0        0  1.134849  0.907687 -0.416385 -0.966814   \n",
       "1        0        2        0  0.119282  0.681738  0.272399  0.080113   \n",
       "2        0        1        0  0.779973  0.946463  1.425350 -0.132928   \n",
       "3        0        1        0 -0.734910 -0.274641 -0.438509  0.759097   \n",
       "4        0        2        1 -0.452718 -0.477513  0.972316  0.970731   \n",
       "\n",
       "        g-4       g-5       g-6  ...  g_component_156  g_component_157  \\\n",
       "0 -0.254723 -1.017473 -1.364787  ...         1.964606        -0.919256   \n",
       "1  1.205169  0.686517  0.313396  ...        -0.944428         0.118273   \n",
       "2 -0.006122  1.492493  0.235577  ...        -1.403747         0.114112   \n",
       "3  2.346330 -0.858153 -2.288417  ...         0.575879        -1.070684   \n",
       "4  1.463427 -0.869555 -0.375501  ...         1.804947        -0.989905   \n",
       "\n",
       "   c_component_0  c_component_1  c_component_2  c_component_3  c_component_4  \\\n",
       "0      -4.919171       1.552054       1.341573       1.461339       0.787781   \n",
       "1      -5.092003      -0.378341      -0.181403       0.941975      -0.512484   \n",
       "2       1.355263       0.297862       0.288732      -0.153444      -0.045587   \n",
       "3      10.936555       1.142666       1.118825      -1.398768       0.073445   \n",
       "4      -3.638540       0.565012       0.700291      -0.199855       0.261442   \n",
       "\n",
       "   c_component_5  c_component_6  c_component_7  \n",
       "0       1.178956      -0.030037       0.090488  \n",
       "1      -0.665863      -0.464563       0.449651  \n",
       "2       0.537705       0.924253       0.060457  \n",
       "3       1.413511      -0.048360       0.520131  \n",
       "4      -0.289355       0.248847      -0.226713  \n",
       "\n",
       "[5 rows x 1178 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041325,
     "end_time": "2020-11-23T00:54:14.526931",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.485606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:14.626067Z",
     "iopub.status.busy": "2020-11-23T00:54:14.620654Z",
     "iopub.status.idle": "2020-11-23T00:54:14.648410Z",
     "shell.execute_reply": "2020-11-23T00:54:14.647798Z"
    },
    "papermill": {
     "duration": 0.078956,
     "end_time": "2020-11-23T00:54:14.648562",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.569606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/bckenstler/CLR/blob/master/clr_callback.py\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:14.743053Z",
     "iopub.status.busy": "2020-11-23T00:54:14.742126Z",
     "iopub.status.idle": "2020-11-23T00:54:14.746228Z",
     "shell.execute_reply": "2020-11-23T00:54:14.745474Z"
    },
    "papermill": {
     "duration": 0.055247,
     "end_time": "2020-11-23T00:54:14.746356",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.691109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(yt, yp, smoothing_rate):\n",
    "    p_min = smoothing_rate\n",
    "    p_max = 1 - p_min\n",
    "    yp = np.clip(yp, p_min, p_max)\n",
    "    return log_loss(yt, yp, labels=[0,1])\n",
    "\n",
    "def metric(y_true, y_pred, targets, smoothing_rate):\n",
    "    metrics = []\n",
    "    for _target in targets:\n",
    "        metrics.append(loss_fn(y_true.loc[:, _target],\n",
    "                               y_pred.loc[:, _target].astype(float),\n",
    "                               smoothing_rate))\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:14.843031Z",
     "iopub.status.busy": "2020-11-23T00:54:14.841935Z",
     "iopub.status.idle": "2020-11-23T00:54:14.845472Z",
     "shell.execute_reply": "2020-11-23T00:54:14.844761Z"
    },
    "papermill": {
     "duration": 0.058126,
     "end_time": "2020-11-23T00:54:14.845599",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.787473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(num_inputs, num_outputs, hidden_units, dropout_rate,\n",
    "                 smoothing_rate, learning_rate):\n",
    "\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Input(num_inputs)])\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    actv = 'relu'\n",
    "\n",
    "    for hidden_size in hidden_units:\n",
    "        \n",
    "        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_size, activation=actv)))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_outputs, activation=\"sigmoid\")))\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr = learning_rate, weight_decay = 1e-5, clipvalue = 756), \n",
    "                 loss=BinaryCrossentropy(label_smoothing=smoothing_rate),\n",
    "                 )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def create_model(num_inputs, num_outputs, hidden_units, dropout_rate,\n",
    "#                  smoothing_rate, learning_rate):\n",
    "    \n",
    "#     inp = tf.keras.layers.Input(shape = (num_inputs, ))\n",
    "#     x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    \n",
    "#     for units in hidden_units:\n",
    "        \n",
    "#         x = tf.keras.layers.Dense(units, activation = 'relu')(x)\n",
    "#         x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "#         x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "#     out = tf.keras.layers.Dense(num_outputs, activation = 'sigmoid')(x)\n",
    "#     model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "#     LR = learning_rate * strategy.num_replicas_in_sync\n",
    "    \n",
    "#     model.compile(optimizer = tf.optimizers.Adam(LR),\n",
    "#                   loss=BinaryCrossentropy(label_smoothing=smoothing_rate))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:14.937624Z",
     "iopub.status.busy": "2020-11-23T00:54:14.936829Z",
     "iopub.status.idle": "2020-11-23T00:54:14.939678Z",
     "shell.execute_reply": "2020-11-23T00:54:14.939066Z"
    },
    "papermill": {
     "duration": 0.051542,
     "end_time": "2020-11-23T00:54:14.939802",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.888260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transfer_weight(model_source ,model_dest):\n",
    "    print('Transfering weights')\n",
    "    for i in range(len(model_source.layers[:-1])):\n",
    "        model_dest.layers[i].set_weights(model_source.layers[i].get_weights())\n",
    "    \n",
    "    print('Done')\n",
    "    return model_dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045489,
     "end_time": "2020-11-23T00:54:15.027139",
     "exception": false,
     "start_time": "2020-11-23T00:54:14.981650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tune Parameters\n",
    "\n",
    "https://www.kaggle.com/gogo827jz/hyperparameter-tuning-for-neural-network-on-tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:15.125092Z",
     "iopub.status.busy": "2020-11-23T00:54:15.124222Z",
     "iopub.status.idle": "2020-11-23T00:54:15.136276Z",
     "shell.execute_reply": "2020-11-23T00:54:15.136880Z"
    },
    "papermill": {
     "duration": 0.066597,
     "end_time": "2020-11-23T00:54:15.137064",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.070467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:15.231354Z",
     "iopub.status.busy": "2020-11-23T00:54:15.230404Z",
     "iopub.status.idle": "2020-11-23T00:54:15.234347Z",
     "shell.execute_reply": "2020-11-23T00:54:15.235024Z"
    },
    "papermill": {
     "duration": 0.055699,
     "end_time": "2020-11-23T00:54:15.235187",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.179488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:15.367006Z",
     "iopub.status.busy": "2020-11-23T00:54:15.366193Z",
     "iopub.status.idle": "2020-11-23T00:54:15.369227Z",
     "shell.execute_reply": "2020-11-23T00:54:15.369777Z"
    },
    "papermill": {
     "duration": 0.076909,
     "end_time": "2020-11-23T00:54:15.369945",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.293036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimise(params):\n",
    "    \n",
    "    hidden_units = [params['hidden_unit_1'], params['hidden_unit_2'], params['hidden_unit_3']]\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    learning_rate = params['learning_rate']\n",
    "    smoothing_rate = params['smoothing_rate']\n",
    "    nfolds = 3\n",
    "    \n",
    "    column_idx = [i for i in range(train_x.shape[1])]\n",
    "    features = list(train_x.columns)\n",
    "\n",
    "    losses = []\n",
    "    res_nn = train_y.copy()\n",
    "    res_nn.loc[:, train_y.columns] = 0\n",
    "    \n",
    "    seed = 42\n",
    "    tf.random.set_seed(seed)\n",
    "    stratified_k_fold = IterativeStratification(n_splits=nfolds, order=1, random_state = seed)\n",
    "    for n, (tr, val) in enumerate(stratified_k_fold.split(train_x, train_y)):\n",
    "\n",
    "        checkpoint_path = f'Tuning:Seed:{seed}:Fold:{n}.hdf5'\n",
    "        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose=0,\n",
    "                                     save_best_only = True, save_weights_only = True, mode = 'min')\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=1e-6, patience=3,\n",
    "                                           verbose=0, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience=10,\n",
    "                              verbose=0)\n",
    "\n",
    "        with strategy.scope():\n",
    "            model = create_model(len(features), len(train_y.columns),\n",
    "                                 hidden_units=hidden_units, dropout_rate=dropout_rate,\n",
    "                                smoothing_rate=smoothing_rate, learning_rate=learning_rate)\n",
    "\n",
    "        history = model.fit(train_x.values[tr][:, column_idx],\n",
    "                            train_y.values[tr],\n",
    "                            validation_data=(train_x.values[val][:, column_idx], train_y.values[val]),\n",
    "                            epochs=60, batch_size = 128 * strategy.num_replicas_in_sync, \n",
    "                            callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
    "                            verbose=0\n",
    "                             )\n",
    "        hist = pd.DataFrame(history.history)\n",
    "\n",
    "        val_predictions = model.predict(train_x.values[val][:, column_idx])\n",
    "        res_nn.loc[val, train_y.columns] += val_predictions\n",
    "        res_nn.loc[train_x['cp_type'] == 1, train_y.columns] = 0\n",
    "\n",
    "        loss = metric(train_y.loc[val, train_y.columns], \n",
    "                      res_nn.loc[val, train_y.columns],\n",
    "                      targets=list(train_y.columns),\n",
    "                      smoothing_rate=smoothing_rate)\n",
    "        losses.append(loss)\n",
    "        print(f'OOF Metric For SEED {seed} + FOLD {n+1} : {loss}')\n",
    "        print('+-' * 10)\n",
    "        \n",
    "        K.clear_session()\n",
    "        del model, history, hist\n",
    "        x = gc.collect()\n",
    "\n",
    "    print(losses)\n",
    "    return np.mean(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:15.464826Z",
     "iopub.status.busy": "2020-11-23T00:54:15.464033Z",
     "iopub.status.idle": "2020-11-23T00:54:15.467550Z",
     "shell.execute_reply": "2020-11-23T00:54:15.466784Z"
    },
    "papermill": {
     "duration": 0.051439,
     "end_time": "2020-11-23T00:54:15.467676",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.416237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param_space = {'hidden_unit_1': hp.choice('hidden_unit_1', [4096, 2048, 1024, 512]), \n",
    "#                'hidden_unit_2': hp.choice('hidden_unit_2', [4096, 2048, 1024, 512]), \n",
    "#                'hidden_unit_3': hp.choice('hidden_unit_3', [4096, 2048, 1024, 512]), \n",
    "#                'dropout_rate': hp.uniform('dropout_rate', 0, 0.5),\n",
    "#                'learning_rate': hp.uniform('learning_rate', 1e-5, 1e-2),\n",
    "#                'smoothing_rate': hp.uniform('smoothing_rate', 1e-15, 1e-10)\n",
    "#               }\n",
    "\n",
    "# trials = Trials()\n",
    "\n",
    "# hopt = fmin(fn = optimise, \n",
    "#             space = param_space, \n",
    "#             algo = tpe.suggest, \n",
    "#             max_evals = 15, \n",
    "#             timeout = 2.9 * 60 * 60, \n",
    "#             trials = trials, \n",
    "#            )\n",
    "\n",
    "# print(hopt)\n",
    "# \"\"\"\n",
    "# {'dropout_rate': 0.48730130590765064, 'hidden_unit_1': 2, 'hidden_unit_2': 2,\n",
    "# 'hidden_unit_3': 3, 'learning_rate': 0.005462817520018712, 'smoothing_rate': 1.568544957309706e-11}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042423,
     "end_time": "2020-11-23T00:54:15.554583",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.512160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train + Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:15.652239Z",
     "iopub.status.busy": "2020-11-23T00:54:15.651337Z",
     "iopub.status.idle": "2020-11-23T00:54:15.809664Z",
     "shell.execute_reply": "2020-11-23T00:54:15.808987Z"
    },
    "papermill": {
     "duration": 0.209812,
     "end_time": "2020-11-23T00:54:15.809795",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.599983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#HIDDEN_UNITS = [248, 1024, 512]\n",
    "HIDDEN_UNITS = [1024, 1024, 1024]\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "SMOOTHING_RATE = 1e-15\n",
    "\n",
    "submission_predictions = submission.copy()\n",
    "submission_predictions.loc[:, train_y.columns] = 0\n",
    "\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "nonscored_historys = dict()\n",
    "historys = dict()\n",
    "column_idx = [i for i in range(train_x.shape[1])]\n",
    "\n",
    "# nonscored_clr = CyclicLR(base_lr=1e-3, max_lr=0.006, step_size=2000, mode='triangular')\n",
    "scored_clr = CyclicLR(base_lr=1e-3, max_lr=0.006, step_size=2000, mode='triangular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:15.904533Z",
     "iopub.status.busy": "2020-11-23T00:54:15.903385Z",
     "iopub.status.idle": "2020-11-23T00:54:15.909695Z",
     "shell.execute_reply": "2020-11-23T00:54:15.908941Z"
    },
    "papermill": {
     "duration": 0.056303,
     "end_time": "2020-11-23T00:54:15.909821",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.853518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # train on nonscored first for transfer learning\n",
    "# print('Training on nonscored')\n",
    "\n",
    "# train_y_nonscored.loc[train_features['cp_type']=='ctl_vehicle', train_y_nonscored.columns] = 0\n",
    "# # pulled from eda notebook\n",
    "# high_corr_columns = ['abc_transporter_expression_enhancer',\n",
    "#                      'diacylglycerol_o_acyltransferase_inhibitor',\n",
    "#                      'dna_methyltransferase_inhibitor',\n",
    "#                      'dna_methyltransferase_inhibitor',\n",
    "#                      'glucocorticoid_receptor_antagonist',\n",
    "#                      'glutathione_reductase_(nadph)_activators',\n",
    "#                      'glutathione_reductase_(nadph)_activators',\n",
    "#                      'heme_oxygenase_activators',\n",
    "#                      'heme_oxygenase_activators',\n",
    "#                      'hiv_protease_inhibitor',\n",
    "#                      'keap1_ligand',\n",
    "#                      'macrophage_migration_inhibiting_factor_inhibitor',\n",
    "#                      'nfkb_activator',\n",
    "#                      'quorum_sensing_signaling_modulator',\n",
    "#                      'ror_inverse_agonist',\n",
    "#                      'sars_coronavirus_3c-like_protease_inhibitor',\n",
    "#                      'steryl_sulfatase_inhibitor']\n",
    "\n",
    "# target_nonscored = train_y_nonscored[high_corr_columns].copy()\n",
    "\n",
    "# X_train_nonscored, y_train_nonscored, X_test_nonscored, y_test_nonscored = iterative_train_test_split(train_x.values, target_nonscored.values, 0.1)\n",
    "# print(f\"======{y_train_nonscored.shape}========{y_test_nonscored.shape}=====\")\n",
    "\n",
    "# checkpoint_path = f'nonscored_model.hdf5'\n",
    "# cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose=1,\n",
    "#                              save_best_only = True, save_weights_only = True, mode = 'min')\n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=1e-6, patience=3,\n",
    "#                                    verbose=1, mode='min')\n",
    "# early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience=10,\n",
    "#                       verbose=1)\n",
    "\n",
    "# model_nonscored = create_model(X_train_nonscored.shape[1], y_train_nonscored.shape[1],\n",
    "#                               hidden_units=HIDDEN_UNITS, dropout_rate=DROPOUT_RATE)\n",
    "# nonscored_history = model_nonscored.fit(X_train_nonscored, y_train_nonscored,\n",
    "#                                         validation_data=(X_test_nonscored, y_test_nonscored),\n",
    "#                                         epochs=60, batch_size=128,\n",
    "#                                         callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
    "#                                         verbose=1\n",
    "#                      )\n",
    "# nonscored_historys[f'nonscored_history_SEED={seed}_FOLD={n+1}'] = nonscored_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T00:54:16.029004Z",
     "iopub.status.busy": "2020-11-23T00:54:16.005834Z",
     "iopub.status.idle": "2020-11-23T01:49:36.900257Z",
     "shell.execute_reply": "2020-11-23T01:49:36.900906Z"
    },
    "papermill": {
     "duration": 3320.947873,
     "end_time": "2020-11-23T01:49:36.901089",
     "exception": false,
     "start_time": "2020-11-23T00:54:15.953216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.2949\n",
      "Epoch 00001: val_loss improved from inf to 0.03583, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.2949 - val_loss: 0.0358\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 00002: val_loss improved from 0.03583 to 0.02184, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0267 - val_loss: 0.0218\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 00003: val_loss improved from 0.02184 to 0.01914, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0202 - val_loss: 0.0191\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0185\n",
      "Epoch 00004: val_loss improved from 0.01914 to 0.01787, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 00005: val_loss improved from 0.01787 to 0.01744, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0175 - val_loss: 0.0174\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00006: val_loss improved from 0.01744 to 0.01731, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0167 - val_loss: 0.0173\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 00007: val_loss did not improve from 0.01731\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0158 - val_loss: 0.0173\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00008: val_loss improved from 0.01731 to 0.01713, saving model to repeat:Seed:779:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0150 - val_loss: 0.0171\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00009: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0143 - val_loss: 0.0174\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0132\n",
      "Epoch 00010: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0132 - val_loss: 0.0181\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00011: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0122 - val_loss: 0.0180\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0115\n",
      "Epoch 00012: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0115 - val_loss: 0.0187\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0104\n",
      "Epoch 00013: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0104 - val_loss: 0.0191\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00014: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0096 - val_loss: 0.0207\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00015: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0088 - val_loss: 0.0199\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00016: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0074 - val_loss: 0.0211\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00017: val_loss did not improve from 0.01713\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0060 - val_loss: 0.0228\n",
      "Epoch 18/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00018: val_loss did not improve from 0.01713\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0050 - val_loss: 0.0230\n",
      "Epoch 00018: early stopping\n",
      "OOF Metric For ITER 1 + SEED 779 + FOLD 1 : 0.015660938625499627\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 00001: val_loss improved from inf to 0.02052, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.1032 - val_loss: 0.0205\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0194\n",
      "Epoch 00002: val_loss improved from 0.02052 to 0.01843, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0194 - val_loss: 0.0184\n",
      "Epoch 3/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0179\n",
      "Epoch 00003: val_loss improved from 0.01843 to 0.01762, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0179 - val_loss: 0.0176\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0170\n",
      "Epoch 00004: val_loss improved from 0.01762 to 0.01722, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0170 - val_loss: 0.0172\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00005: val_loss improved from 0.01722 to 0.01700, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.0161 - val_loss: 0.0170\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00006: val_loss improved from 0.01700 to 0.01695, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 90ms/step - loss: 0.0151 - val_loss: 0.0169\n",
      "Epoch 7/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00007: val_loss improved from 0.01695 to 0.01689, saving model to repeat:Seed:779:Fold:1.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0143 - val_loss: 0.0169\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0128\n",
      "Epoch 00008: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0128 - val_loss: 0.0171\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 00009: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0113 - val_loss: 0.0176\n",
      "Epoch 10/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00010: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0097 - val_loss: 0.0181\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00011: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0080 - val_loss: 0.0186\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0068\n",
      "Epoch 00012: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0068 - val_loss: 0.0193\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0063\n",
      "Epoch 00013: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0063 - val_loss: 0.0200\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0062\n",
      "Epoch 00014: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0062 - val_loss: 0.0206\n",
      "Epoch 15/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Epoch 00015: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0062 - val_loss: 0.0212\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00016: val_loss did not improve from 0.01689\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0060 - val_loss: 0.0220\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00017: val_loss did not improve from 0.01689\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0058 - val_loss: 0.0220\n",
      "Epoch 00017: early stopping\n",
      "OOF Metric For ITER 1 + SEED 779 + FOLD 2 : 0.015615585925020057\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1369\n",
      "Epoch 00001: val_loss improved from inf to 0.02111, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 14s 99ms/step - loss: 0.1369 - val_loss: 0.0211\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0198\n",
      "Epoch 00002: val_loss improved from 0.02111 to 0.01845, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0198 - val_loss: 0.0184\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 00003: val_loss improved from 0.01845 to 0.01774, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0181 - val_loss: 0.0177\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00004: val_loss improved from 0.01774 to 0.01740, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0172 - val_loss: 0.0174\n",
      "Epoch 5/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00005: val_loss improved from 0.01740 to 0.01729, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0165 - val_loss: 0.0173\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00006: val_loss improved from 0.01729 to 0.01715, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0157 - val_loss: 0.0172\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00007: val_loss improved from 0.01715 to 0.01711, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0150 - val_loss: 0.0171\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00008: val_loss improved from 0.01711 to 0.01711, saving model to repeat:Seed:779:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0140 - val_loss: 0.0171\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0129\n",
      "Epoch 00009: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0129 - val_loss: 0.0180\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0119\n",
      "Epoch 00010: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 12s 83ms/step - loss: 0.0119 - val_loss: 0.0180\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0104\n",
      "Epoch 00011: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0104 - val_loss: 0.0189\n",
      "Epoch 12/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00012: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0088 - val_loss: 0.0197\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0076\n",
      "Epoch 00013: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0076 - val_loss: 0.0207\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0061\n",
      "Epoch 00014: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 13s 92ms/step - loss: 0.0061 - val_loss: 0.0214\n",
      "Epoch 15/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00015: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0050 - val_loss: 0.0224\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00016: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0040 - val_loss: 0.0240\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 00017: val_loss did not improve from 0.01711\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0033 - val_loss: 0.0243\n",
      "Epoch 18/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00018: val_loss did not improve from 0.01711\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 10s 76ms/step - loss: 0.0028 - val_loss: 0.0249\n",
      "Epoch 00018: early stopping\n",
      "OOF Metric For ITER 1 + SEED 779 + FOLD 3 : 0.015775644166825163\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19052, 206)========(4762, 206)=====\n",
      "Epoch 1/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.1523\n",
      "Epoch 00001: val_loss improved from inf to 0.02256, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 83ms/step - loss: 0.1522 - val_loss: 0.0226\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 00002: val_loss improved from 0.02256 to 0.01910, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0207 - val_loss: 0.0191\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00003: val_loss improved from 0.01910 to 0.01810, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0187 - val_loss: 0.0181\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0177\n",
      "Epoch 00004: val_loss improved from 0.01810 to 0.01755, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0177 - val_loss: 0.0175\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00005: val_loss improved from 0.01755 to 0.01741, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0169 - val_loss: 0.0174\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0163\n",
      "Epoch 00006: val_loss improved from 0.01741 to 0.01712, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0163 - val_loss: 0.0171\n",
      "Epoch 7/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00007: val_loss improved from 0.01712 to 0.01705, saving model to repeat:Seed:779:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 8/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00008: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0151 - val_loss: 0.0171\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0144\n",
      "Epoch 00009: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0144 - val_loss: 0.0171\n",
      "Epoch 10/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0136\n",
      "Epoch 00010: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0136 - val_loss: 0.0174\n",
      "Epoch 11/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0126\n",
      "Epoch 00011: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0126 - val_loss: 0.0177\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0116\n",
      "Epoch 00012: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 10s 75ms/step - loss: 0.0116 - val_loss: 0.0185\n",
      "Epoch 13/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00013: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0106 - val_loss: 0.0188\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00014: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0096 - val_loss: 0.0196\n",
      "Epoch 15/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 00015: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0086 - val_loss: 0.0197\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00016: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0075 - val_loss: 0.0209\n",
      "Epoch 17/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00017: val_loss did not improve from 0.01705\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 76ms/step - loss: 0.0073 - val_loss: 0.0209\n",
      "Epoch 00017: early stopping\n",
      "OOF Metric For ITER 1 + SEED 779 + FOLD 4 : 0.015732292176893654\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00001: val_loss improved from inf to 0.02035, saving model to repeat:Seed:779:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0989 - val_loss: 0.0203\n",
      "Epoch 2/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0193\n",
      "Epoch 00002: val_loss improved from 0.02035 to 0.01831, saving model to repeat:Seed:779:Fold:4.hdf5\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0193 - val_loss: 0.0183\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0181\n",
      "Epoch 00003: val_loss improved from 0.01831 to 0.01765, saving model to repeat:Seed:779:Fold:4.hdf5\n",
      "138/138 [==============================] - 14s 99ms/step - loss: 0.0181 - val_loss: 0.0176\n",
      "Epoch 4/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0174\n",
      "Epoch 00004: val_loss improved from 0.01765 to 0.01719, saving model to repeat:Seed:779:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 5/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00005: val_loss improved from 0.01719 to 0.01694, saving model to repeat:Seed:779:Fold:4.hdf5\n",
      "138/138 [==============================] - 10s 76ms/step - loss: 0.0165 - val_loss: 0.0169\n",
      "Epoch 6/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00006: val_loss did not improve from 0.01694\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 7/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00007: val_loss improved from 0.01694 to 0.01691, saving model to repeat:Seed:779:Fold:4.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0150 - val_loss: 0.0169\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00008: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0139 - val_loss: 0.0171\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0127\n",
      "Epoch 00009: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0127 - val_loss: 0.0174\n",
      "Epoch 10/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0116\n",
      "Epoch 00010: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0116 - val_loss: 0.0179\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0102\n",
      "Epoch 00011: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 10s 76ms/step - loss: 0.0102 - val_loss: 0.0186\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0087\n",
      "Epoch 00012: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0087 - val_loss: 0.0196\n",
      "Epoch 13/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00013: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 10s 76ms/step - loss: 0.0075 - val_loss: 0.0204\n",
      "Epoch 14/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Epoch 00014: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 10s 75ms/step - loss: 0.0061 - val_loss: 0.0210\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00015: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0050 - val_loss: 0.0217\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00016: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 83ms/step - loss: 0.0041 - val_loss: 0.0222\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 00017: val_loss did not improve from 0.01691\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0034 - val_loss: 0.0233\n",
      "Epoch 00017: early stopping\n",
      "OOF Metric For ITER 1 + SEED 779 + FOLD 5 : 0.01564035793893677\n",
      "+-+-+-+-+-+-+-+-+-+-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.2913\n",
      "Epoch 00001: val_loss improved from inf to 0.03772, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.2913 - val_loss: 0.0377\n",
      "Epoch 2/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0268\n",
      "Epoch 00002: val_loss improved from 0.03772 to 0.02184, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0268 - val_loss: 0.0218\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 00003: val_loss improved from 0.02184 to 0.01906, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0206 - val_loss: 0.0191\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00004: val_loss improved from 0.01906 to 0.01820, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0187 - val_loss: 0.0182\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 00005: val_loss did not improve from 0.01820\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0180 - val_loss: 0.0200\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00006: val_loss improved from 0.01820 to 0.01727, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0172 - val_loss: 0.0173\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00007: val_loss improved from 0.01727 to 0.01690, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0162 - val_loss: 0.0169\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00008: val_loss improved from 0.01690 to 0.01688, saving model to repeat:Seed:577:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00009: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 13s 96ms/step - loss: 0.0149 - val_loss: 0.0169\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00010: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0139 - val_loss: 0.0176\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00011: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0130 - val_loss: 0.0176\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00012: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0124 - val_loss: 0.0188\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 00013: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0113 - val_loss: 0.0184\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0103\n",
      "Epoch 00014: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0103 - val_loss: 0.0194\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00015: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 12s 90ms/step - loss: 0.0094 - val_loss: 0.0194\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0083\n",
      "Epoch 00016: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0083 - val_loss: 0.0209\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 00017: val_loss did not improve from 0.01688\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0070 - val_loss: 0.0217\n",
      "Epoch 18/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 00018: val_loss did not improve from 0.01688\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0059 - val_loss: 0.0220\n",
      "Epoch 00018: early stopping\n",
      "OOF Metric For ITER 2 + SEED 577 + FOLD 1 : 0.015556574011013674\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 00001: val_loss improved from inf to 0.02045, saving model to repeat:Seed:577:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.1042 - val_loss: 0.0205\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 00002: val_loss improved from 0.02045 to 0.01814, saving model to repeat:Seed:577:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0193 - val_loss: 0.0181\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0179\n",
      "Epoch 00003: val_loss improved from 0.01814 to 0.01748, saving model to repeat:Seed:577:Fold:1.hdf5\n",
      "138/138 [==============================] - 11s 76ms/step - loss: 0.0179 - val_loss: 0.0175\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00004: val_loss improved from 0.01748 to 0.01716, saving model to repeat:Seed:577:Fold:1.hdf5\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0169 - val_loss: 0.0172\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00005: val_loss improved from 0.01716 to 0.01695, saving model to repeat:Seed:577:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0161 - val_loss: 0.0169\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00006: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 11s 76ms/step - loss: 0.0151 - val_loss: 0.0170\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00007: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0140 - val_loss: 0.0170\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0127\n",
      "Epoch 00008: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0127 - val_loss: 0.0175\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00009: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0110 - val_loss: 0.0177\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00010: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0092 - val_loss: 0.0183\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00011: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0075 - val_loss: 0.0189\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0061\n",
      "Epoch 00012: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0061 - val_loss: 0.0199\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0056\n",
      "Epoch 00013: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0056 - val_loss: 0.0206\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0056\n",
      "Epoch 00014: val_loss did not improve from 0.01695\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0056 - val_loss: 0.0214\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00015: val_loss did not improve from 0.01695\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0055 - val_loss: 0.0215\n",
      "Epoch 00015: early stopping\n",
      "OOF Metric For ITER 2 + SEED 577 + FOLD 2 : 0.015583839122170257\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.1621\n",
      "Epoch 00001: val_loss improved from inf to 0.02207, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.1620 - val_loss: 0.0221\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 00002: val_loss improved from 0.02207 to 0.01933, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0205 - val_loss: 0.0193\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00003: val_loss improved from 0.01933 to 0.01846, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 13s 91ms/step - loss: 0.0187 - val_loss: 0.0185\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 00004: val_loss improved from 0.01846 to 0.01760, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 15s 112ms/step - loss: 0.0176 - val_loss: 0.0176\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00005: val_loss improved from 0.01760 to 0.01752, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0168 - val_loss: 0.0175\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00006: val_loss did not improve from 0.01752\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0162 - val_loss: 0.0175\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00007: val_loss improved from 0.01752 to 0.01726, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0172 - val_loss: 0.0173\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00008: val_loss improved from 0.01726 to 0.01700, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0160 - val_loss: 0.0170\n",
      "Epoch 9/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00009: val_loss improved from 0.01700 to 0.01698, saving model to repeat:Seed:577:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0150 - val_loss: 0.0170\n",
      "Epoch 10/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0145\n",
      "Epoch 00010: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0145 - val_loss: 0.0172\n",
      "Epoch 11/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00011: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0142 - val_loss: 0.0173\n",
      "Epoch 12/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00012: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0130 - val_loss: 0.0176\n",
      "Epoch 13/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00013: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0121 - val_loss: 0.0183\n",
      "Epoch 14/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 00014: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0111 - val_loss: 0.0183\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0100\n",
      "Epoch 00015: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0100 - val_loss: 0.0190\n",
      "Epoch 16/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0089\n",
      "Epoch 00016: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0089 - val_loss: 0.0198\n",
      "Epoch 17/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 00017: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0079 - val_loss: 0.0204\n",
      "Epoch 18/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00018: val_loss did not improve from 0.01698\n",
      "138/138 [==============================] - 10s 75ms/step - loss: 0.0069 - val_loss: 0.0213\n",
      "Epoch 19/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Epoch 00019: val_loss did not improve from 0.01698\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0062 - val_loss: 0.0221\n",
      "Epoch 00019: early stopping\n",
      "OOF Metric For ITER 2 + SEED 577 + FOLD 3 : 0.01569996535134547\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1402\n",
      "Epoch 00001: val_loss improved from inf to 0.02158, saving model to repeat:Seed:577:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.1402 - val_loss: 0.0216\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0202\n",
      "Epoch 00002: val_loss improved from 0.02158 to 0.01925, saving model to repeat:Seed:577:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0202 - val_loss: 0.0193\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 00003: val_loss improved from 0.01925 to 0.01812, saving model to repeat:Seed:577:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0184 - val_loss: 0.0181\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 00004: val_loss improved from 0.01812 to 0.01747, saving model to repeat:Seed:577:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0174 - val_loss: 0.0175\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00005: val_loss improved from 0.01747 to 0.01720, saving model to repeat:Seed:577:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0165 - val_loss: 0.0172\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00006: val_loss improved from 0.01720 to 0.01705, saving model to repeat:Seed:577:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0156 - val_loss: 0.0171\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00007: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 13s 94ms/step - loss: 0.0148 - val_loss: 0.0171\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00008: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0143 - val_loss: 0.0172\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00009: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0134 - val_loss: 0.0175\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0126\n",
      "Epoch 00010: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0126 - val_loss: 0.0180\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0118\n",
      "Epoch 00011: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 90ms/step - loss: 0.0118 - val_loss: 0.0184\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00012: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0106 - val_loss: 0.0192\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00013: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0097 - val_loss: 0.0195\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00014: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0088 - val_loss: 0.0202\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 00015: val_loss did not improve from 0.01705\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0078 - val_loss: 0.0207\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00016: val_loss did not improve from 0.01705\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0072 - val_loss: 0.0216\n",
      "Epoch 00016: early stopping\n",
      "OOF Metric For ITER 2 + SEED 577 + FOLD 4 : 0.015708559211276615\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19052, 206)========(4762, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 00001: val_loss improved from inf to 0.02089, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 13s 96ms/step - loss: 0.1076 - val_loss: 0.0209\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0194\n",
      "Epoch 00002: val_loss improved from 0.02089 to 0.01868, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0194 - val_loss: 0.0187\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 00003: val_loss improved from 0.01868 to 0.01764, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0180 - val_loss: 0.0176\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00004: val_loss improved from 0.01764 to 0.01759, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0171 - val_loss: 0.0176\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00005: val_loss improved from 0.01759 to 0.01713, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0162 - val_loss: 0.0171\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00006: val_loss improved from 0.01713 to 0.01712, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0152 - val_loss: 0.0171\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00007: val_loss improved from 0.01712 to 0.01701, saving model to repeat:Seed:577:Fold:4.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0142 - val_loss: 0.0170\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0131\n",
      "Epoch 00008: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0131 - val_loss: 0.0174\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0117\n",
      "Epoch 00009: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0117 - val_loss: 0.0186\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0100\n",
      "Epoch 00010: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0100 - val_loss: 0.0223\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00011: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0088 - val_loss: 0.0196\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 00012: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0071 - val_loss: 0.0210\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00013: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 14s 99ms/step - loss: 0.0058 - val_loss: 0.0215\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00014: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0044 - val_loss: 0.0234\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 00015: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0034 - val_loss: 0.0235\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00016: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0028 - val_loss: 0.0240\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00017: val_loss did not improve from 0.01701\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0022 - val_loss: 0.0248\n",
      "Epoch 00017: early stopping\n",
      "OOF Metric For ITER 2 + SEED 577 + FOLD 5 : 0.01566336343232678\n",
      "+-+-+-+-+-+-+-+-+-+-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.2223\n",
      "Epoch 00001: val_loss improved from inf to 0.03028, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 14s 99ms/step - loss: 0.2223 - val_loss: 0.0303\n",
      "Epoch 2/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0247\n",
      "Epoch 00002: val_loss improved from 0.03028 to 0.02156, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 13s 93ms/step - loss: 0.0247 - val_loss: 0.0216\n",
      "Epoch 3/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0208\n",
      "Epoch 00003: val_loss improved from 0.02156 to 0.01973, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0208 - val_loss: 0.0197\n",
      "Epoch 4/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0191\n",
      "Epoch 00004: val_loss improved from 0.01973 to 0.01834, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0191 - val_loss: 0.0183\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 00005: val_loss improved from 0.01834 to 0.01783, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0180 - val_loss: 0.0178\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 00006: val_loss improved from 0.01783 to 0.01734, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.0174 - val_loss: 0.0173\n",
      "Epoch 7/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0176\n",
      "Epoch 00007: val_loss did not improve from 0.01734\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0176 - val_loss: 0.0174\n",
      "Epoch 8/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00008: val_loss improved from 0.01734 to 0.01727, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0166 - val_loss: 0.0173\n",
      "Epoch 9/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00009: val_loss improved from 0.01727 to 0.01701, saving model to repeat:Seed:402:Fold:0.hdf5\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0161 - val_loss: 0.0170\n",
      "Epoch 10/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00010: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 77ms/step - loss: 0.0157 - val_loss: 0.0173\n",
      "Epoch 11/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0154\n",
      "Epoch 00011: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0154 - val_loss: 0.0170\n",
      "Epoch 12/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0146\n",
      "Epoch 00012: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0146 - val_loss: 0.0180\n",
      "Epoch 13/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00013: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 10s 75ms/step - loss: 0.0141 - val_loss: 0.0173\n",
      "Epoch 14/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0138\n",
      "Epoch 00014: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0139 - val_loss: 0.0176\n",
      "Epoch 15/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0133\n",
      "Epoch 00015: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0133 - val_loss: 0.0176\n",
      "Epoch 16/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0129\n",
      "Epoch 00016: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0129 - val_loss: 0.0178\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0119\n",
      "Epoch 00017: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0119 - val_loss: 0.0182\n",
      "Epoch 18/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00018: val_loss did not improve from 0.01701\n",
      "138/138 [==============================] - 12s 90ms/step - loss: 0.0110 - val_loss: 0.0185\n",
      "Epoch 19/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0098\n",
      "Epoch 00019: val_loss did not improve from 0.01701\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 12s 91ms/step - loss: 0.0098 - val_loss: 0.0192\n",
      "Epoch 00019: early stopping\n",
      "OOF Metric For ITER 3 + SEED 402 + FOLD 1 : 0.015744083851702322\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00001: val_loss improved from inf to 0.02010, saving model to repeat:Seed:402:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0995 - val_loss: 0.0201\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 00002: val_loss improved from 0.02010 to 0.01813, saving model to repeat:Seed:402:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0191 - val_loss: 0.0181\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0178\n",
      "Epoch 00003: val_loss did not improve from 0.01813\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0178 - val_loss: 0.0183\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00004: val_loss improved from 0.01813 to 0.01713, saving model to repeat:Seed:402:Fold:1.hdf5\n",
      "138/138 [==============================] - 13s 92ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00005: val_loss improved from 0.01713 to 0.01699, saving model to repeat:Seed:402:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0159 - val_loss: 0.0170\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00006: val_loss improved from 0.01699 to 0.01691, saving model to repeat:Seed:402:Fold:1.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0148 - val_loss: 0.0169\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0135\n",
      "Epoch 00007: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0135 - val_loss: 0.0172\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0120\n",
      "Epoch 00008: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 14s 102ms/step - loss: 0.0120 - val_loss: 0.0175\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0103\n",
      "Epoch 00009: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 14s 104ms/step - loss: 0.0103 - val_loss: 0.0181\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0085\n",
      "Epoch 00010: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 14s 99ms/step - loss: 0.0085 - val_loss: 0.0189\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0068\n",
      "Epoch 00011: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0068 - val_loss: 0.0196\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00012: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0052 - val_loss: 0.0201\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00013: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0042 - val_loss: 0.0206\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00014: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0038 - val_loss: 0.0217\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0039\n",
      "Epoch 00015: val_loss did not improve from 0.01691\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0039 - val_loss: 0.0222\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00016: val_loss did not improve from 0.01691\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 13s 92ms/step - loss: 0.0043 - val_loss: 0.0230\n",
      "Epoch 00016: early stopping\n",
      "OOF Metric For ITER 3 + SEED 402 + FOLD 2 : 0.01548047095857186\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1618\n",
      "Epoch 00001: val_loss improved from inf to 0.02248, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 12s 90ms/step - loss: 0.1618 - val_loss: 0.0225\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 00002: val_loss improved from 0.02248 to 0.01916, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 15s 108ms/step - loss: 0.0207 - val_loss: 0.0192\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00003: val_loss improved from 0.01916 to 0.01843, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 14s 101ms/step - loss: 0.0187 - val_loss: 0.0184\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0176\n",
      "Epoch 00004: val_loss improved from 0.01843 to 0.01761, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0176 - val_loss: 0.0176\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00005: val_loss improved from 0.01761 to 0.01725, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0168 - val_loss: 0.0173\n",
      "Epoch 6/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00006: val_loss improved from 0.01725 to 0.01710, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0165 - val_loss: 0.0171\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00007: val_loss did not improve from 0.01710\n",
      "138/138 [==============================] - 11s 79ms/step - loss: 0.0157 - val_loss: 0.0175\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00008: val_loss did not improve from 0.01710\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0171 - val_loss: 0.0173\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00009: val_loss improved from 0.01710 to 0.01704, saving model to repeat:Seed:402:Fold:2.hdf5\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0155 - val_loss: 0.0170\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 00010: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0146 - val_loss: 0.0174\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0138\n",
      "Epoch 00011: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0131\n",
      "Epoch 00012: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0131 - val_loss: 0.0177\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0119\n",
      "Epoch 00013: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0119 - val_loss: 0.0183\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 00014: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0108 - val_loss: 0.0186\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0098\n",
      "Epoch 00015: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 15s 108ms/step - loss: 0.0098 - val_loss: 0.0193\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00016: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 15s 108ms/step - loss: 0.0088 - val_loss: 0.0199\n",
      "Epoch 17/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0077\n",
      "Epoch 00017: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 11s 83ms/step - loss: 0.0077 - val_loss: 0.0206\n",
      "Epoch 18/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0068\n",
      "Epoch 00018: val_loss did not improve from 0.01704\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0068 - val_loss: 0.0218\n",
      "Epoch 19/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00019: val_loss did not improve from 0.01704\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 78ms/step - loss: 0.0060 - val_loss: 0.0221\n",
      "Epoch 00019: early stopping\n",
      "OOF Metric For ITER 3 + SEED 402 + FOLD 3 : 0.015740382979345414\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19051, 206)========(4763, 206)=====\n",
      "Epoch 1/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.1403\n",
      "Epoch 00001: val_loss improved from inf to 0.02161, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.1402 - val_loss: 0.0216\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0205\n",
      "Epoch 00002: val_loss improved from 0.02161 to 0.01903, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0205 - val_loss: 0.0190\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 00003: val_loss improved from 0.01903 to 0.01803, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 13s 91ms/step - loss: 0.0186 - val_loss: 0.0180\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0177\n",
      "Epoch 00004: val_loss improved from 0.01803 to 0.01748, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0177 - val_loss: 0.0175\n",
      "Epoch 5/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00005: val_loss improved from 0.01748 to 0.01710, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00006: val_loss improved from 0.01710 to 0.01690, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 12s 89ms/step - loss: 0.0159 - val_loss: 0.0169\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0153\n",
      "Epoch 00007: val_loss improved from 0.01690 to 0.01684, saving model to repeat:Seed:402:Fold:3.hdf5\n",
      "138/138 [==============================] - 13s 92ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0147\n",
      "Epoch 00008: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0147 - val_loss: 0.0169\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00009: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0141 - val_loss: 0.0173\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0133\n",
      "Epoch 00010: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0133 - val_loss: 0.0176\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00011: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0130 - val_loss: 0.0178\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0117\n",
      "Epoch 00012: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 12s 88ms/step - loss: 0.0117 - val_loss: 0.0179\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0107\n",
      "Epoch 00013: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0107 - val_loss: 0.0187\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0100\n",
      "Epoch 00014: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0100 - val_loss: 0.0192\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0093\n",
      "Epoch 00015: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0093 - val_loss: 0.0199\n",
      "Epoch 16/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00016: val_loss did not improve from 0.01684\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0081 - val_loss: 0.0205\n",
      "Epoch 17/26\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0076\n",
      "Epoch 00017: val_loss did not improve from 0.01684\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 11s 80ms/step - loss: 0.0076 - val_loss: 0.0207\n",
      "Epoch 00017: early stopping\n",
      "OOF Metric For ITER 3 + SEED 402 + FOLD 4 : 0.01555137193076251\n",
      "+-+-+-+-+-+-+-+-+-+-\n",
      "Training actual model\n",
      "======(19052, 206)========(4762, 206)=====\n",
      "Epoch 1/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00001: val_loss improved from inf to 0.01998, saving model to repeat:Seed:402:Fold:4.hdf5\n",
      "138/138 [==============================] - 15s 112ms/step - loss: 0.1018 - val_loss: 0.0200\n",
      "Epoch 2/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 00002: val_loss improved from 0.01998 to 0.01832, saving model to repeat:Seed:402:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0191 - val_loss: 0.0183\n",
      "Epoch 3/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0179\n",
      "Epoch 00003: val_loss improved from 0.01832 to 0.01812, saving model to repeat:Seed:402:Fold:4.hdf5\n",
      "138/138 [==============================] - 13s 95ms/step - loss: 0.0179 - val_loss: 0.0181\n",
      "Epoch 4/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00004: val_loss improved from 0.01812 to 0.01714, saving model to repeat:Seed:402:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 87ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 5/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00005: val_loss improved from 0.01714 to 0.01688, saving model to repeat:Seed:402:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 83ms/step - loss: 0.0160 - val_loss: 0.0169\n",
      "Epoch 6/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00006: val_loss improved from 0.01688 to 0.01683, saving model to repeat:Seed:402:Fold:4.hdf5\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0151 - val_loss: 0.0168\n",
      "Epoch 7/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0140\n",
      "Epoch 00007: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 11s 82ms/step - loss: 0.0140 - val_loss: 0.0169\n",
      "Epoch 8/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0127\n",
      "Epoch 00008: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 11s 81ms/step - loss: 0.0127 - val_loss: 0.0177\n",
      "Epoch 9/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 00009: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0113 - val_loss: 0.0178\n",
      "Epoch 10/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0098\n",
      "Epoch 00010: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 13s 92ms/step - loss: 0.0098 - val_loss: 0.0191\n",
      "Epoch 11/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00011: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 12s 85ms/step - loss: 0.0081 - val_loss: 0.0197\n",
      "Epoch 12/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0066\n",
      "Epoch 00012: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 13s 93ms/step - loss: 0.0066 - val_loss: 0.0210\n",
      "Epoch 13/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00013: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 13s 94ms/step - loss: 0.0052 - val_loss: 0.0218\n",
      "Epoch 14/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00014: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 14s 101ms/step - loss: 0.0040 - val_loss: 0.0229\n",
      "Epoch 15/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0032\n",
      "Epoch 00015: val_loss did not improve from 0.01683\n",
      "138/138 [==============================] - 12s 84ms/step - loss: 0.0032 - val_loss: 0.0236\n",
      "Epoch 16/26\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00016: val_loss did not improve from 0.01683\n",
      "Restoring model weights from the end of the best epoch.\n",
      "138/138 [==============================] - 12s 86ms/step - loss: 0.0024 - val_loss: 0.0242\n",
      "Epoch 00016: early stopping\n",
      "OOF Metric For ITER 3 + SEED 402 + FOLD 5 : 0.015472469076382325\n",
      "+-+-+-+-+-+-+-+-+-+-\n"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "ITERATIONS = 3\n",
    "\n",
    "for iteration in range(ITERATIONS):\n",
    "    \n",
    "    seed = random.randint(1, 1000)\n",
    "    #seed = 42 # test\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    stratified_k_fold = IterativeStratification(n_splits=NFOLDS, order=1, random_state = seed)\n",
    "    for n, (tr, val) in enumerate(stratified_k_fold.split(train_x, train_y)):\n",
    "\n",
    "\n",
    "        checkpoint_path = f'repeat:Seed:{seed}:Fold:{n}.hdf5'\n",
    "        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose=1,\n",
    "                                     save_best_only = True, save_weights_only = True, mode = 'min')\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=1e-6, patience=3,\n",
    "                                           verbose=1, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience=10,\n",
    "                              verbose=1)\n",
    "\n",
    "        # train on nonscored first for transfer learning\n",
    "#         print('Training on nonscored')\n",
    "#         print(f\"======{train_y_nonscored.values[tr].shape}========{train_y_nonscored.values[val].shape}=====\")\n",
    "#         model_nonscored = create_model(len(features), len(train_y_nonscored.columns),\n",
    "#                                       hidden_units=HIDDEN_UNITS, dropout_rate=DROPOUT_RATE)\n",
    "#         nonscored_history = model_nonscored.fit(train_x.values[tr][:, column_idx],\n",
    "#                                                 train_y_nonscored.values[tr],\n",
    "#                                                 validation_data=(train_x.values[val][:, column_idx], train_y_nonscored.values[val]),\n",
    "#                                                 epochs=30, batch_size=128,\n",
    "#                                                 #callbacks=[reduce_lr_loss, cb_checkpt, early, nonscored_clr],\n",
    "#                                                 callbacks=[cb_checkpt, early, nonscored_clr],\n",
    "#                                                 verbose=1\n",
    "#                              )\n",
    "#         nonscored_historys[f'nonscored_history_SEED={seed}_FOLD={n+1}'] = nonscored_history\n",
    "\n",
    "        #model_nonscored.load_weights(checkpoint_path)\n",
    "\n",
    "        # train actual model\n",
    "        print('Training actual model')\n",
    "        print(f\"======{train_y.values[tr].shape}========{train_y.values[val].shape}=====\")\n",
    "        tr_feature = train_x.iloc[tr, column_idx].copy().reset_index(drop=True)\n",
    "        tr_target = train_y.iloc[tr,:].copy().reset_index(drop=True)\n",
    "        val_feature = train_x.iloc[val, column_idx].copy().reset_index(drop=True)\n",
    "        val_target = train_y.iloc[val,:].copy().reset_index(drop=True)\n",
    "        \n",
    "        # process out control rows when training\n",
    "        train_tr_target = tr_target[tr_feature['cp_type']!=1].copy().reset_index(drop=True)\n",
    "        train_tr_feature = tr_feature[tr_feature['cp_type']!=1].copy().reset_index(drop=True)\n",
    "        train_val_target = val_target[val_feature['cp_type']!=1].copy().reset_index(drop=True)\n",
    "        train_val_feature = val_feature[val_feature['cp_type']!=1].copy().reset_index(drop=True)\n",
    "        train_tr_feature.drop('cp_type', axis=1, inplace=True)\n",
    "        train_val_feature.drop('cp_type', axis=1, inplace=True)\n",
    "\n",
    "        model = create_model(train_tr_feature.shape[1], len(train_y.columns),\n",
    "                             hidden_units=HIDDEN_UNITS, dropout_rate=DROPOUT_RATE,\n",
    "                            smoothing_rate=SMOOTHING_RATE, learning_rate=LEARNING_RATE)\n",
    "        \n",
    "        #model = transfer_weight(model_nonscored, model)\n",
    "        \n",
    "        history = model.fit(train_tr_feature, train_tr_target,\n",
    "                            validation_data=(train_val_feature, train_val_target),\n",
    "                            epochs=26, batch_size=128,\n",
    "                            #callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
    "                            callbacks=[cb_checkpt, early, scored_clr],\n",
    "                            verbose=1\n",
    "                             )\n",
    "        historys[f'history_SEED={seed}_FOLD={n+1}'] = history\n",
    "\n",
    "        train_predictions = tr_target.copy()\n",
    "        train_predictions.loc[:, train_y.columns] = 0\n",
    "        tr_idx = tr_feature.loc[tr_feature['cp_type']==0, list(train_tr_feature.columns)].index\n",
    "        train_predict = model.predict(tr_feature.loc[tr_idx, list(train_tr_feature.columns)])\n",
    "        \n",
    "        val_predictions = val_target.copy()\n",
    "        val_predictions.loc[:, train_y.columns] = 0\n",
    "        val_idx = val_feature.loc[val_feature['cp_type']==0, list(train_tr_feature.columns)].index\n",
    "        val_predict = model.predict(val_feature.loc[val_idx,list(train_tr_feature.columns)])\n",
    "        \n",
    "        test_idx = test.loc[test['cp_type']==0, list(train_tr_feature.columns)].index\n",
    "        test_predict = model.predict(test.loc[test_idx, list(train_tr_feature.columns)])\n",
    "\n",
    "        train_predictions.loc[tr_idx, train_y.columns] += train_predict\n",
    "        val_predictions.loc[val_idx, train_y.columns] += val_predict\n",
    "        submission_predictions.loc[test_idx, train_y.columns] += test_predict\n",
    "\n",
    "        #val_predict = pd.DataFrame(val_predict, columns=train_y.columns)\n",
    "        #val_predict.loc[val_feature['cp_type']==1, train_y.columns] = 0\n",
    "\n",
    "        tr_loss = metric(tr_target, train_predictions,\n",
    "                      targets=list(train_y.columns),\n",
    "                      smoothing_rate=SMOOTHING_RATE)\n",
    "        train_losses.append(tr_loss)\n",
    "\n",
    "        val_loss = metric(val_target, val_predictions,\n",
    "                      targets=list(train_y.columns),\n",
    "                      smoothing_rate=SMOOTHING_RATE)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'OOF Metric For ITER {iteration+1} + SEED {seed} + FOLD {n+1} : {val_loss}')\n",
    "        print('+-' * 10)\n",
    "\n",
    "train_predictions.loc[:, train_y.columns] /= (NFOLDS*ITERATIONS)\n",
    "submission_predictions.loc[:, train_y.columns] /= (NFOLDS*ITERATIONS)\n",
    "\n",
    "#train_predictions.loc[train_features['cp_type']=='ctl_vehicle',  train_y.columns] = 0\n",
    "#submission_predictions.loc[test_features['cp_type']=='ctl_vehicle',  train_y.columns] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T01:50:07.180640Z",
     "iopub.status.busy": "2020-11-23T01:50:07.179558Z",
     "iopub.status.idle": "2020-11-23T01:50:07.182807Z",
     "shell.execute_reply": "2020-11-23T01:50:07.182068Z"
    },
    "papermill": {
     "duration": 15.06449,
     "end_time": "2020-11-23T01:50:07.183121",
     "exception": false,
     "start_time": "2020-11-23T01:49:52.118631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Show Model loss in plots\n",
    "# for k,v in historys.items():\n",
    "#     loss = []\n",
    "#     val_loss = []\n",
    "#     loss.append(v.history['loss'][:40])\n",
    "#     val_loss.append(v.history['val_loss'][:40])\n",
    "    \n",
    "\n",
    "# plt.figure(figsize = (15, 6))\n",
    "# plt.plot(np.mean(loss, axis=0))\n",
    "# plt.plot(np.mean(val_loss, axis=0))\n",
    "# plt.yscale('log')\n",
    "# plt.yticks(ticks=[1,1e-1,1e-2])\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Average Logloss')\n",
    "# plt.legend(['Training','Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T01:50:37.653273Z",
     "iopub.status.busy": "2020-11-23T01:50:37.652547Z",
     "iopub.status.idle": "2020-11-23T01:50:37.656078Z",
     "shell.execute_reply": "2020-11-23T01:50:37.655464Z"
    },
    "papermill": {
     "duration": 15.208454,
     "end_time": "2020-11-23T01:50:37.656212",
     "exception": false,
     "start_time": "2020-11-23T01:50:22.447758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# h = scored_clr.history\n",
    "# lr = h['lr']\n",
    "# loss = h['loss']\n",
    "# iterations = h['iterations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T01:51:07.887923Z",
     "iopub.status.busy": "2020-11-23T01:51:07.886736Z",
     "iopub.status.idle": "2020-11-23T01:51:07.890828Z",
     "shell.execute_reply": "2020-11-23T01:51:07.890073Z"
    },
    "papermill": {
     "duration": 15.066146,
     "end_time": "2020-11-23T01:51:07.890954",
     "exception": false,
     "start_time": "2020-11-23T01:50:52.824808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try filtering out control before predicting\n",
    "# try remove cluster features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T01:51:38.581010Z",
     "iopub.status.busy": "2020-11-23T01:51:38.579977Z",
     "iopub.status.idle": "2020-11-23T01:51:38.583693Z",
     "shell.execute_reply": "2020-11-23T01:51:38.584412Z"
    },
    "papermill": {
     "duration": 15.035033,
     "end_time": "2020-11-23T01:51:38.584625",
     "exception": false,
     "start_time": "2020-11-23T01:51:23.549592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train loss: 0.01265366657967562\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean train loss: {np.mean(train_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T01:52:09.897024Z",
     "iopub.status.busy": "2020-11-23T01:52:09.895950Z",
     "iopub.status.idle": "2020-11-23T01:52:09.899801Z",
     "shell.execute_reply": "2020-11-23T01:52:09.900602Z"
    },
    "papermill": {
     "duration": 15.995881,
     "end_time": "2020-11-23T01:52:09.900781",
     "exception": false,
     "start_time": "2020-11-23T01:51:53.904900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss: 0.015641726583871498\n"
     ]
    }
   ],
   "source": [
    "print('Mean validation loss:', np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T01:52:40.284244Z",
     "iopub.status.busy": "2020-11-23T01:52:40.282417Z",
     "iopub.status.idle": "2020-11-23T01:52:42.913873Z",
     "shell.execute_reply": "2020-11-23T01:52:42.913212Z"
    },
    "papermill": {
     "duration": 17.716334,
     "end_time": "2020-11-23T01:52:42.914019",
     "exception": false,
     "start_time": "2020-11-23T01:52:25.197685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_predictions.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 3623.452013,
   "end_time": "2020-11-23T01:52:58.154674",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-23T00:52:34.702661",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
